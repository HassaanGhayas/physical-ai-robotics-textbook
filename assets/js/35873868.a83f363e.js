"use strict";(globalThis.webpackChunkmy_web=globalThis.webpackChunkmy_web||[]).push([[3519],{6611:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"book/ai-integration/index","title":"AI Integration","description":"Integrating artificial intelligence systems with humanoid robotics","source":"@site/docs/book/ai-integration/index.md","sourceDirName":"book/ai-integration","slug":"/book/ai-integration/","permalink":"/physical-ai-robotics-textbook/docs/book/ai-integration/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/book/ai-integration/index.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"AI Integration","sidebar_position":5,"description":"Integrating artificial intelligence systems with humanoid robotics"}}');var o=i(4848),r=i(8453);const a={title:"AI Integration",sidebar_position:5,description:"Integrating artificial intelligence systems with humanoid robotics"},s="AI Integration",c={},l=[{value:"Hardware Requirements for AI",id:"hardware-requirements-for-ai",level:2},{value:"Vision-Language-Action Models",id:"vision-language-action-models",level:2},{value:"Technical Architecture",id:"technical-architecture",level:2},{value:"Deep Reinforcement Learning for Locomotion",id:"deep-reinforcement-learning-for-locomotion",level:2},{value:"Policy Networks",id:"policy-networks",level:3},{value:"Training Considerations",id:"training-considerations",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",technicaldiagrams:"technicaldiagrams",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"import HardwareSpecs from '@site/src/components/HardwareSpecs/HardwareSpecs';\nimport CodeExamples from '@site/src/components/CodeExamples/CodeExamples';\nimport TechnicalDiagrams from '@site/src/components/TechnicalDiagrams/TechnicalDiagrams';"}),"\n",(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"ai-integration",children:"AI Integration"})}),"\n",(0,o.jsx)(n.p,{children:"This module focuses on integrating artificial intelligence systems with humanoid robotics. We'll explore how to implement neural networks, computer vision, and natural language processing to create intelligent robotic agents."}),"\n",(0,o.jsx)(n.h2,{id:"hardware-requirements-for-ai",children:"Hardware Requirements for AI"}),"\n",(0,o.jsx)(n.p,{children:'<HardwareSpecs\nname="AI Acceleration Hardware"\ncategory="Machine Learning Accelerators"\nspecs={{\nprocessor: "NVIDIA Jetson AGX Orin (64GB)",\ncores: "12-core ARM v8.4 64-bit CPU",\ngpu: "2048-core NVIDIA Ampere GPU",\nmemory: "64GB LPDDR5",\ncompute: "275 TOPS (INT8)",\npower: "60W typical, 100W max"\n}}\ncost={{\nprice: "$1,499",\nreasoning: "High-performance AI processing for real-time inference on humanoid robots"\n}}\npros={[\n"Extremely high AI performance per watt",\n"Real-time inference capabilities",\n"Compact form factor suitable for humanoid robots",\n"Supports all major AI frameworks"\n]}\ncons={[\n"Significant power requirements",\n"Requires active cooling",\n"High cost",\n"Complex thermal management"\n]}\n/>'}),"\n",(0,o.jsx)(n.h2,{id:"vision-language-action-models",children:"Vision-Language-Action Models"}),"\n",(0,o.jsx)(n.p,{children:"The integration of vision, language, and action models enables humanoid robots to understand and interact with their environment in natural ways. This section explores the implementation of VLA models for robotic control."}),"\n",(0,o.jsx)(n.p,{children:'<CodeExamples\ntitle="Vision-Language-Action Inference"\ndescription="Sample implementation of a VLA model that processes visual input and generates robotic actions"\nlanguage="python"\ncode={`import torch\nimport numpy as np\nfrom transformers import CLIPProcessor, CLIPModel\nimport rospy\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge'}),"\n",(0,o.jsxs)(n.p,{children:["class VisionLanguageAction:\ndef ",(0,o.jsx)(n.strong,{children:"init"}),'(self):\n# Load pre-trained CLIP model for vision-language understanding\nself.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\nself.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\nself.bridge = CvBridge()']}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'    # Initialize ROS node\n    rospy.init_node(\'vla_controller\')\n\n    # Publishers and subscribers\n    self.image_sub = rospy.Subscriber(\'/camera/rgb/image_raw\', Image, self.image_callback)\n    self.cmd_pub = rospy.Publisher(\'/cmd_vel\', Twist, queue_size=1)\n\n    # Define action commands\n    self.actions = [\n        "move forward",\n        "turn left",\n        "turn right",\n        "stop",\n        "pick up object",\n        "place object"\n    ]\n\ndef image_callback(self, data):\n    # Convert ROS image to OpenCV format\n    cv_image = self.bridge.imgmsg_to_cv2(data, "bgr8")\n\n    # Process image and text with CLIP\n    inputs = self.processor(\n        text=self.actions,\n        images=cv_image,\n        return_tensors="pt",\n        padding=True\n    )\n\n    # Get similarity scores\n    outputs = self.model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=-1).detach().numpy()\n\n    # Determine the most likely action\n    best_action_idx = np.argmax(probs)\n    best_action = self.actions[best_action_idx]\n    confidence = probs[0][best_action_idx]\n\n    # Execute action if confidence is high enough\n    if confidence > 0.7:\n        self.execute_action(best_action)\n\ndef execute_action(self, action):\n    cmd = Twist()\n\n    if action == "move forward":\n        cmd.linear.x = 0.5\n    elif action == "turn left":\n        cmd.angular.z = 0.5\n    elif action == "turn right":\n        cmd.angular.z = -0.5\n    elif action == "stop":\n        pass  # Twist is already zero\n    # Add more actions as needed\n\n    self.cmd_pub.publish(cmd)\n    rospy.loginfo(f"Executing action: {action} with confidence {confidence:.2f}")\n'})}),"\n",(0,o.jsxs)(n.p,{children:["if ",(0,o.jsx)(n.strong,{children:"name"})," == '",(0,o.jsx)(n.strong,{children:"main"}),'\':\nvla = VisionLanguageAction()\ntry:\nrospy.spin()\nexcept KeyboardInterrupt:\nprint("Shutting down VLA controller")`}\ncopyable={true}\nexpandable={true}\nmaxHeight="400px"\n/>']}),"\n",(0,o.jsx)(n.h2,{id:"technical-architecture",children:"Technical Architecture"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.technicaldiagrams,{title:"AI Integration Architecture",description:"High-level architecture showing how AI models integrate with the robotic control system",imageurl:"/img/ai-integration-architecture.png",caption:"Architecture diagram showing the flow from sensors through AI models to actuators",alttext:"AI integration architecture diagram",interactive:"{true}",zoomable:"{true}"})}),"\n",(0,o.jsx)(n.h2,{id:"deep-reinforcement-learning-for-locomotion",children:"Deep Reinforcement Learning for Locomotion"}),"\n",(0,o.jsx)(n.p,{children:"Deep reinforcement learning (DRL) is crucial for enabling humanoid robots to learn complex locomotion patterns. This approach allows robots to adapt their movement strategies based on environmental feedback."}),"\n",(0,o.jsx)(n.h3,{id:"policy-networks",children:"Policy Networks"}),"\n",(0,o.jsx)(n.p,{children:"Policy networks in DRL determine the actions a humanoid robot should take based on its current state. These networks are typically implemented as deep neural networks that map sensor inputs to motor commands."}),"\n",(0,o.jsx)(n.h3,{id:"training-considerations",children:"Training Considerations"}),"\n",(0,o.jsx)(n.p,{children:"Training DRL policies for humanoid robots requires careful consideration of simulation-to-reality transfer, safety constraints, and reward function design."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var t=i(6540);const o={},r=t.createContext(o);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);