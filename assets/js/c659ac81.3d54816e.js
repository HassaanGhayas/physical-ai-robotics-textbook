"use strict";(globalThis.webpackChunkmy_web=globalThis.webpackChunkmy_web||[]).push([[5767],{3418:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>c,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>r});const o=JSON.parse('{"id":"book/vision-language-action/index","title":"Vision-Language-Action (VLA)","description":"The convergence of LLMs and Robotics for humanoid applications","source":"@site/docs/book/vision-language-action/index.md","sourceDirName":"book/vision-language-action","slug":"/book/vision-language-action/","permalink":"/physical-ai-robotics-textbook/docs/book/vision-language-action/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/book/vision-language-action/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Vision-Language-Action (VLA)","sidebar_position":1,"description":"The convergence of LLMs and Robotics for humanoid applications"},"sidebar":"bookSidebar","previous":{"title":"Nav2 Path Planning","permalink":"/physical-ai-robotics-textbook/docs/book/ai-robot-brain/nav2-path-planning"},"next":{"title":"Voice-to-Action","permalink":"/physical-ai-robotics-textbook/docs/book/vision-language-action/voice-to-action"}}');var t=i(4848),s=i(8453);const a={title:"Vision-Language-Action (VLA)",sidebar_position:1,description:"The convergence of LLMs and Robotics for humanoid applications"},c="Vision-Language-Action (VLA)",l={},r=[{value:"Overview",id:"overview",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:3},{value:"Cognitive Planning",id:"cognitive-planning",level:3},{value:"Vision Integration",id:"vision-integration",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:2},{value:"Prerequisites",id:"prerequisites",level:2}];function d(n){const e={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(e.p,{children:"Focus: The convergence of LLMs and Robotics."}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"Vision-Language-Action (VLA) systems represent the cutting edge of robotics, where large language models are integrated with perception and action systems. This enables robots to understand natural language commands, perceive their environment, and execute complex sequences of actions."}),"\n",(0,t.jsx)(e.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsx)(e.p,{children:"This module covers:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Voice-to-Action"}),": Using OpenAI Whisper for voice commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cognitive Planning"}),': Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Capstone Project"}),": The Autonomous Humanoid - A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsx)(e.h3,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Speech recognition using Whisper"}),"\n",(0,t.jsx)(e.li,{children:"Natural language understanding"}),"\n",(0,t.jsx)(e.li,{children:"Intent classification"}),"\n",(0,t.jsx)(e.li,{children:"Action sequence generation"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"cognitive-planning",children:"Cognitive Planning"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Language grounding in physical world"}),"\n",(0,t.jsx)(e.li,{children:"Task decomposition from high-level commands"}),"\n",(0,t.jsx)(e.li,{children:"Context-aware action selection"}),"\n",(0,t.jsx)(e.li,{children:"Execution monitoring and error recovery"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"vision-integration",children:"Vision Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Object detection and recognition"}),"\n",(0,t.jsx)(e.li,{children:"Spatial reasoning"}),"\n",(0,t.jsx)(e.li,{children:"Scene understanding"}),"\n",(0,t.jsx)(e.li,{children:"Multi-modal fusion"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this module, you will:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Implement voice-to-action systems for humanoid robots"}),"\n",(0,t.jsx)(e.li,{children:"Design cognitive planning systems that translate natural language to robot actions"}),"\n",(0,t.jsx)(e.li,{children:"Integrate vision systems with language understanding"}),"\n",(0,t.jsx)(e.li,{children:"Build a complete VLA system for humanoid robotics"}),"\n",(0,t.jsx)(e.li,{children:"Create a capstone project demonstrating autonomous humanoid behavior"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,t.jsx)(e.p,{children:"The capstone project brings together all concepts learned in this module:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Receive a voice command through speech recognition"}),"\n",(0,t.jsx)(e.li,{children:"Plan a path using cognitive reasoning"}),"\n",(0,t.jsx)(e.li,{children:"Navigate obstacles using perception systems"}),"\n",(0,t.jsx)(e.li,{children:"Identify objects using computer vision"}),"\n",(0,t.jsx)(e.li,{children:"Manipulate objects using action execution"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Basic understanding of language models"}),"\n",(0,t.jsx)(e.li,{children:"Familiarity with computer vision concepts"}),"\n",(0,t.jsx)(e.li,{children:"Experience with ROS 2 action systems"}),"\n",(0,t.jsx)(e.li,{children:"Understanding of planning algorithms"}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>c});var o=i(6540);const t={},s=o.createContext(t);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);